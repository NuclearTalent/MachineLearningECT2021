{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5UDKqBOW0F8Q"
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "In this notebook, you will be training Convolutional Neural Networks (CNNs) for image classification tasks. We will walk through the technicalities of building a CNN with TensorFlow's high-level Keras API as well as some practical tips for training CNNs and evaluating their performace.\n",
    "\n",
    "If you are using Google Colab, make sure you are using a GPU-enabled runtime. Go to \"Runtime\" $\\rightarrow$ \"Change runtime type\", then make sure \"GPU\" is selected for \"Hardware accelerator\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vSQSsrX90F8S"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First we import the Python packages that we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aG4lWWJ40F8U"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# This is simply an alias for convenience\n",
    "layers = tf.keras.layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define some utility functions that will be helpful. You don't need to look at these too closely, unless you are curious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jSQ1yJec0F8w"
   },
   "outputs": [],
   "source": [
    "def get_attpc_class(label):\n",
    "    \"\"\"Gets the class name for a given label.\n",
    "    \n",
    "    Arguments:\n",
    "        label (int): The integer target label.\n",
    "        \n",
    "    Returns:\n",
    "        The name of the class that corresponds to the given label.\n",
    "    \"\"\"\n",
    "    return ['proton', 'carbon', 'other'][label]\n",
    "\n",
    "def load_attpc_data():\n",
    "    \"\"\"Loads in the AT-TPC data.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of the form ((real_features, real_targets), (simulated_features, simulated_targets))\n",
    "    \"\"\"\n",
    "    simulated_data_origin = 'https://github.com/CompPhysics/MachineLearningMSU/raw/master/Day2_materials/data/simulated-attpc-events.h5'\n",
    "    real_data_origin = 'https://github.com/CompPhysics/MachineLearningMSU/raw/master/Day2_materials/data/real-attpc-events.h5'\n",
    "    \n",
    "    simulated_path = tf.keras.utils.get_file('simulated-attpc-data.h5', origin=simulated_data_origin)\n",
    "    real_path = tf.keras.utils.get_file('real-attpc-data.h5', origin=real_data_origin)\n",
    "    \n",
    "    with h5py.File(simulated_path, 'r') as h5:\n",
    "        simulated_features = h5['features'][:]\n",
    "        simulated_targets = h5['targets'][:]\n",
    "        \n",
    "    with h5py.File(real_path, 'r') as h5:\n",
    "        real_features = h5['features'][:]\n",
    "        real_targets = h5['targets'][:]\n",
    "    \n",
    "    return (real_features, real_targets), (simulated_features, simulated_targets)\n",
    "\n",
    "def plot_learning_curve(history):\n",
    "    \"\"\"Plots a learning curve from a training history.\n",
    "    \n",
    "    Arguments:\n",
    "        history (dict): The training history returned by `model.fit()`.\n",
    "        \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(11, 6), dpi=100)\n",
    "    plt.plot(history.history['loss'], 'o-', label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], 'o:', color='r', label='Validation Loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xticks(range(0, len(history.history['loss'])), range(1, len(history.history['loss']) + 1))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_confusion_matrix(y_true,\n",
    "                          y_pred,\n",
    "                          classes,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"This function prints and plots the confusion matrix.\n",
    "    \n",
    "    Adapted from:\n",
    "    https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \n",
    "    Arguments:\n",
    "        y_true: Real class labels.\n",
    "        y_pred: Predicted class labels.\n",
    "        classes: List of class names.\n",
    "        title: Title for the plot.\n",
    "        cmap: Colormap to be used.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        title = 'Confusion matrix'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 4), dpi=100)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha='center', va='center',\n",
    "                    color='white' if cm[i, j] > thresh else 'black')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UeCyhkUa0F8Y"
   },
   "source": [
    "## Classifying Handwritten Digits\n",
    "\n",
    "We are going to start by training a model on the [MNIST database of handwritten digits](http://yann.lecun.com/exdb/mnist/). MNIST is a classic dataset consisting of 70,000 images of handwritten digits (0 - 9). Our goal is to create a classifier that can identify the digit based on the image alone. This turns out to be a very easy problem (our CNN should have >99% accuracy), so it makes for a very good introduction to CNNs as we can easily verify that everything is working as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aczQtAu80F8Z"
   },
   "source": [
    "### Data exploration and prepreprocessing\n",
    "\n",
    "First, we need to load the data. TensorFlow makes this easy by including MNIST as one of the datasets that they provide easy access to through the `tf.keras.datasets` API. As a side note, many machine learning applications rely on a few standard datasets that are easily accessible through `tf.keras.datasets` and [TensorFlow Datasets](https://www.tensorflow.org/datasets). These are great resources that can often save lots of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kajx7Bu20F8a"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WiIz4Mea0F8d"
   },
   "source": [
    "The data is conveniently already split in to a training set of 60,000 and a test set of 10,000. Now lets take a closer look at the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jp85LuNF0F8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      "   Shape: (60000, 28, 28)\n",
      "   Type: uint8\n",
      "\n",
      "Training Targets:\n",
      "   Shape: (60000,)\n",
      "   Type: uint8\n"
     ]
    }
   ],
   "source": [
    "print('Training Features:\\n   Shape: {}\\n   Type: {}\\n'.format(x_train.shape, x_train.dtype))\n",
    "print('Training Targets:\\n   Shape: {}\\n   Type: {}'.format(y_train.shape, y_train.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h31aE9BL0F8i"
   },
   "source": [
    "Each of the 60,000 training examples is a 28 x 28 grayscale image with pixel values in the range 0 - 255. We visualize the first 25 examples and their corresponding labels below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "C9djE9X30F8i"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(y_train[i])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTeHEmgI0F8l"
   },
   "source": [
    "Now we must preprocess the data. While it doesn't actually make much a difference for the MNIST task, in general it is critical to perform some kind of normalization of your data before handing it off to a model. It can have a suprising large effect on a model's perforamce and make a model require less sensitive to hyperparameters. We will scale the pixel values to the range [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l5fGtRMS0F8m"
   },
   "outputs": [],
   "source": [
    "x_train = (x_train - 127.5) / 127.5\n",
    "x_test = (x_test - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m32E58Pd0F8n"
   },
   "source": [
    "There is one final detail we need to take care of before proceeding. Images are generally three dimensional because there are three color channels. Because the MNIST data is grayscale, it is technically only two dimensional (one color channel), but the neural network is going to expect three dimensions regardless. So we just need to add an extra dimension to the data so that each example has the shape `(28, 28, 1)` and the network knows how to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKzPA4680F8n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 28, 28, 1)\n",
      "Test data shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.expand_dims(x_train, axis=3)\n",
    "x_test = np.expand_dims(x_test, axis=3)\n",
    "\n",
    "print('Training data shape: {}'.format(x_train.shape))\n",
    "print('Test data shape: {}'.format(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9B6jhiHM0F8p"
   },
   "source": [
    "### Build the model\n",
    "\n",
    "Now we will construct our CNN with TensorFlow's Keras API. Most neural networks are composed of a sequence of feed-forward layers, meaning that each layer is only connected to the ones that come immediately before and after it. The `tf.keras.Sequential` model allows us to easily create such a network by simply defining what kinds of layers should be in the model. We don't have to concern ourselves with how to connect the layers together or how to define the training loop.\n",
    "\n",
    "**In the cell below, construct a CNN with the following architecture:**\n",
    " 1. Convolutional layer with 32 filters, a 4 x 4 kernel, and ReLU activation\n",
    " 2. Max pooling layer with a 2 x 2 pool size\n",
    " 3. Convolutional layer with 32 filters, a 4 x 4 kernel, and ReLU activation\n",
    " 4. Max pooling layer with a 2 x 2 pool size\n",
    " 5. Fully-connected layer with 128 units and ReLU activation\n",
    " 6. Fully-connected layer with an output unit for each class and softmax activation\n",
    " \n",
    "This architecture is a mini version of the novel architecture introduced by [Krizhevsky et al.](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), which set the state-of-the-art on the ImageNet dataset and sparked the rise of CNNs.\n",
    " \n",
    "Some tips:\n",
    " * Use `model.add(<layer>)` to add a new layer to the sequential model.\n",
    " * Use [`layers.Conv2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) for your convolutional layers.\n",
    " * Use [`layers.MaxPool2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) for your pooling layers.\n",
    " * Use [`layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) for your fully-connected layers.\n",
    " * Before adding the first fully-connected layer, the convolutional features must be flattened to a 1D vector. See [`layers.Flatten`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten).\n",
    " * The [`tf.nn`](https://www.tensorflow.org/api_docs/python/tf/nn) module provides access to several common activation functions.\n",
    " * The first layer in the network needs the keyword argument `input_shape` whose value is a tuple that gives the shape of a training example. This shape is already extracted from the data in the cell below, and just needs to be passed to the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OztITb-80F8p"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2215b225f553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# This will print an overview of the network architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/TALENT2020/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   1255\u001b[0m     \"\"\"\n\u001b[1;32m   1256\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[1;32m   1258\u001b[0m                        \u001b[0;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m                        \u001b[0;34m'`fit()` with some data, or specify '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "# We extract the number of classes and the input shape from the data\n",
    "num_classes = len(np.unique(y_train))\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Define a sequential model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# ADD LAYERS HERE\n",
    "\n",
    "\n",
    "\n",
    "# This will print an overview of the network architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCfLY6pb0F8q"
   },
   "source": [
    "Now we can compile the model. We will use the [Adam](https://arxiv.org/pdf/1412.6980.pdf) optimizer (always a safe choice) and train the model to minimize the categorical crossentropy. We also have the model use classification accuracy as a measure of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e-MkTir70F8r"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O8Hvm9pY0F8s"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "Now we can finally train the model. This model should converge relatively quickly, so we only need to train for about 5 epochs. We will use a batch size of 32, which is a common choice and generally works well. Training should only take about a minute on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjzmTaBB0F8t"
   },
   "outputs": [],
   "source": [
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYJMzHt50F8u"
   },
   "source": [
    "You should be able to see loss and accuracy improve as the model trains. If all goes well, training accuracy should be over 99% at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qn0c-u870F8u"
   },
   "source": [
    "### Evaluating the trained model\n",
    "\n",
    "Now that our model has been trained, we can take a closer look at how it performs. The accuracy on the *training* data is very high, but of course that isn't always indicative of how a model will generalize to unseen data. Given enough representational power and training time, a neural network can simply learn to memorize the training data. We will evaluate our model on the held-out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f1DTKXli0F8u"
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test accuracy: {:.04}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-C_vzdW0F8v"
   },
   "source": [
    "The test accuracy should be roughly the same as the training accuracy, which tells us that we have been successful! Because the model performs so well, there is not much need to analyze the performance further. We will do a more thorough evaluation in the next example though.\n",
    "\n",
    "It is also worth mentioning that we didn't have to do any hyperparameter tuning in this example. This is an exception, and very rarely will a model perform so well right off the bat. So don't always expect to see great results on the first try. By the same token, **poor initial results are no indication of potential success.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RhLgmtUH0F8v"
   },
   "source": [
    "# Classifying AT-TPC $^{46}$Ar Data\n",
    "\n",
    "Now we are going to look at some data that comes from the Argon 46 experiments in the AT-TPC. We will be working with 2D projections of reaction products and will train a CNN to try to classify the particle that was produced in the reaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d1FlLVXT0F8w"
   },
   "source": [
    "### Data exploration and preprocessing\n",
    "\n",
    "First we load in the AT-TPC data. We will be working with two datasets: one contains real experimental data from the AT-TPC and the other contains simulated events that were created with [`pytpc`](https://github.com/ATTPC/pytpc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfJhDOPH0F8x"
   },
   "outputs": [],
   "source": [
    "(real_features, real_targets), (sim_features, sim_targets) = load_attpc_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pexS5uWJ0F8y"
   },
   "source": [
    "In both datasets, each image is 128 x 128 and has pixel values in the range 0 - 255. There 50,000 simulated events, and 2,689 real events have been hand-labeled. Notice that this data has not yet been partitioned into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_zCWYtnf0F8y"
   },
   "outputs": [],
   "source": [
    "print('Real Features:\\n   Shape: {}\\n   Type: {}\\n'.format(real_features.shape, real_features.dtype))\n",
    "print('Real Targets:\\n   Shape: {}\\n   Type: {}\\n'.format(real_targets.shape, real_targets.dtype))\n",
    "print('Simulated Features:\\n   Shape: {}\\n   Type: {}\\n'.format(sim_features.shape, sim_features.dtype))\n",
    "print('Simulated Targets:\\n   Shape: {}\\n   Type: {}'.format(sim_targets.shape, sim_targets.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running this notebook on Google Colab, you will not be able to fit all 50,000 simulated events in RAM after they have been normalized. Run the cell below to use only 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_features = sim_features[:10000]\n",
    "sim_targets = sim_targets[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C4ZZqHuU0F80"
   },
   "source": [
    "#### Experimental data\n",
    "\n",
    "We visualize the first 25 examples from the experimental dataset below. Events are classified as either \"proton\", \"carbon\", or \"junk\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dH7fnsgj0F80"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(real_features[i], cmap='gray')\n",
    "    plt.xlabel(get_attpc_class(real_targets[i]))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gc2TxQNJ0F81"
   },
   "source": [
    "#### Simulated data\n",
    "\n",
    "And now we visualize the first 25 examples from the simulated dataset. You'll notice that the noise in the simulated events looks very different from the noise in the real data. This is because there is some systematic structure to the noise in the experimental data, but we are unable to accuractly simulate that noise. This becomes a problem for us later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sl7UeL8F0F81"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(sim_features[i], cmap='gray')\n",
    "    plt.xlabel(get_attpc_class(sim_targets[i]))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z9TdNA4Z0F83"
   },
   "source": [
    "Again, we need to preprocess the data. **In the cell below, normalize the data and add a channel dimension, as we did with the MNIST data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EH09rptU0F83"
   },
   "outputs": [],
   "source": [
    "# NORMALIZE THE DATA\n",
    "real_features = (real_features - 127.5) / 127.5\n",
    "sim_features = (sim_features - 127.5) / 127.5\n",
    "\n",
    "# ADD A CHANNEL DIMENSION\n",
    "real_features = np.expand_dims(real_features, axis=3)\n",
    "sim_features = np.expand_dims(sim_features, axis=3)\n",
    "\n",
    "# Some checks to make sure the data has been correctly preprocessed\n",
    "assert real_features.shape[1:] == (128, 128, 1), 'Real data has incorrect shape'\n",
    "assert sim_features.shape[1:] == (128, 128, 1), 'Simulated data has incorrect shape'\n",
    "assert real_features.min() >= -1 and real_features.max() <= 1, 'Real data is not in the range [-1, 1]'\n",
    "assert sim_features.min() >= -1 and sim_features.max() <= 1, 'Simulated data is not in the range [-1, 1]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mlnFcm9S0F84"
   },
   "source": [
    "### Build the model\n",
    "\n",
    "In this example, we are going to do things a little differently. For the MNIST data, we constructed our own CNN and trained it from scratch. That worked really well for us because it was a small network, small dataset, and relatively easy problem. In practice, designing and training high-performing CNNs from scratch is time consuming and requires large-scale resources. So it is often best not to try to do this yourself. Luckily, companies like Google have already done the heavy lifting for us. TensorFlow provides access to several high-quality networks that have already been trained on datasets with millions of diverse images (such as [ImageNet](http://www.image-net.org)) as general computer-vision models. These complex networks have been trained to identify objects from up 1000 categories. This may not seem immediately useful, but it turns out that training these models on such a large scale forces the networks to learn to extract features are are broadly applicable to many computer vision tasks.\n",
    "\n",
    "So rather than being a hero and training a CNN from scratch, we can instead use the pretrainined convolutional layers from one of these published models and add our own fully-connected classification layers on top. We keep the weights of the convolutional layers frozen, since we know they are already very good at feature extraction, and only update the weights of the top layers, which do the actual classification. This is much faster than training a CNN from scratch, and it can also result in better performance.\n",
    "\n",
    "First, we are going to load in one of these pretrained models. In this example, we are going to use the [VGG16](https://arxiv.org/abs/1409.1556) architecture for image recognition (depicted below).\n",
    "\n",
    "![VGG16 Architecture](https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png)\n",
    "\n",
    "Before we actually load the model, there is one final caveat. The VGG network requires that its input images have three channels. Our images currently only have one, so we will add two more channels with the same values as the first. The new image dimensions should be 128 x 128 x 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UPGzd2-N0F84"
   },
   "outputs": [],
   "source": [
    "real_features = np.repeat(real_features, 3, axis=3)\n",
    "sim_features = np.repeat(sim_features, 3, axis=3)\n",
    "\n",
    "input_shape = real_features.shape[1:]\n",
    "print('Image dimensions: {}'.format(input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DUHAxDxl0F85"
   },
   "source": [
    "In the cell below, we define a function that will load the VGG16 model with the parameter weights that were found by training it on [ImageNet](http://www.image-net.org). We also specify that we do not want to include the fully-connected top layers of the model, since those were not trained to classify our data. We will add our own fully connected layers instead. Unfortunately we can't use `vgg_model.add()` like before because it is not a `Sequential` model. The code below adds two fully-connected hidden layers and an output layer to the end of our VGG16 model using the [Functional API](https://www.tensorflow.org/guide/keras#functional_api). It's still a relatively high-level and intuitive API. Take a moment to read through it and understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "HwE7WiSN0F86"
   },
   "outputs": [],
   "source": [
    "# Determine the number of class labels\n",
    "num_classes = len(np.unique(real_targets))\n",
    "\n",
    "def build_pretrained_vgg_model():\n",
    "    \"\"\"Constructs a CNN with a VGG16's convolutional base and two fully-connected hidden layers on top.\n",
    "    The convolutional base is frozen (the weights can't be updated) and has weights from training on\n",
    "    the ImageNet dataset.\n",
    "    \n",
    "    Returns:\n",
    "        The model.\n",
    "    \"\"\"\n",
    "    # This loads the VGG16 model from TensorFlow with ImageNet weights\n",
    "    vgg_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    \n",
    "    # First we flatten out the features from the VGG16 model\n",
    "    net = layers.Flatten()(vgg_model.output)\n",
    "\n",
    "    # We create a new fully-connected layer that takes the flattened features as its input\n",
    "    net = layers.Dense(512, activation=tf.nn.relu)(net)\n",
    "    # And we add one more hidden layer\n",
    "    net = layers.Dense(512, activation=tf.nn.relu)(net)\n",
    "\n",
    "    # Then we add a final layer which is connected to the previous layer and\n",
    "    # groups our images into one of the three classes\n",
    "    output = layers.Dense(num_classes, activation=tf.nn.softmax)(net)\n",
    "\n",
    "    # Finally, we create a new model whose input is that of the VGG16 model and whose output\n",
    "    # is the final new layer we just created\n",
    "    model = tf.keras.Model(inputs=vgg_model.input, outputs=output)\n",
    "    \n",
    "    # We loop through all layers except the last four and specify that we do not want \n",
    "    # their weights to be updated during training. Again, the weights of the convolutional\n",
    "    # layers have already been trained for general-purpose feature extraction, and we only\n",
    "    # want to update the fully-connected layers that we just added.\n",
    "    for layer in model.layers[:-4]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2iVHN5x0F88"
   },
   "source": [
    "Now we can build and compile our model with the same setup as with the MNIST data. The `learning_rate` parameter simply determines how much the network weights will be updated on each gradient descent step; more on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1i2cE--0F8-"
   },
   "outputs": [],
   "source": [
    "model = build_pretrained_vgg_model()\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.005), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "We are almost ready to train, but first we need to define our learning problem. With the MNIST data, you were provided with training and testing sets that came from the same data distribution. In practice, though, we are not always afforded this luxury. Aquiring large amounts of high-quality labeled data is often very challenging. In fact, the whole reason we want to train a classifier for the AT-TPC is so that humans don't need to hand-label the data! So while we could train the network on the real data, we are instead going to try *transfer learning*. This is when we train a model on one data distribution and try to transfer what it learns to a different distribution. In our case, we will train on the simulated data and save the real data for testing. Labeled simulated data is usually readily available in large quantities, so we want to try to use that to our advantage.\n",
    "\n",
    "One final concept we will introduce here is a *validation set*. This is a third set of data that the network will not see during training, and we can use it to evaluate the performance of the model. Unlike the test set, though, we can use the feedback from the validation data to make adjustments that improve the model's performance. Then the final evaluation will be done with the test data.\n",
    "\n",
    "Now we can train the model. The `validation_split=0.2` argument tells the program to set aside 20% of the traning data for validation. At the end of each epoch, the model will be evaluated on the validation set.\n",
    "\n",
    "Note that training this model will take substantially longer than with the MNIST data. There are a couple of reasons for this. Obviously the network is much larger, and even though we are only training the top layers, there is still an expensive forward pass through the entire network. Also, our AT-TPC images are much larger than the MNIST data, in width, height, and number of channels. This should give you an idea for how these problems scale and why we like to start from networks that have already been trained on millions of images (rather than doing it ourselves).\n",
    "\n",
    "We are going to start by using only 5,000 training examples so that the training time is managable at first (should be about 20 seconds per epoch on a GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6YFOAWH00F8_"
   },
   "outputs": [],
   "source": [
    "model.fit(sim_features[:5000],\n",
    "          sim_targets[:5000],\n",
    "          batch_size=32,\n",
    "          epochs=3,\n",
    "          validation_split=0.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all goes \"right\", you should see the model quickly gets stuck at about 33% training and validation accuracy. Essentially, the model is just making random guesses, since the simulated data is evenly distributed among the three classes. This is a perfect example of how things don't always work right off the bat. We usually need to do some hyperparameter tuning. Out of all the hyperparameters, learning rate is often the most important. If the learning rate is too low or too high, the gradient descent can get stuck at a local minimum or just bounce around.\n",
    "\n",
    "Lets try again, but this time with a lower learning rate. **In the cell below, compile the model with an Adam optimizer with a learning rate of 0.001.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to remake the model in order to reset the weights.\n",
    "# Otherwise, when we call `fit` again, we will be continuing training from where we left off before.\n",
    "model = build_pretrained_vgg_model()\n",
    "\n",
    "# ADD AN ADAM OPTIMIZER WITH A LEARNING RATE OF 0.001\n",
    "model.compile(optimizer=None, \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(sim_features[:5000],\n",
    "          sim_targets[:5000],\n",
    "          batch_size=32,\n",
    "          epochs=3,\n",
    "          validation_split=0.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just this small change, you should see a *huge* improvement in accuracy. The network actually learns something now! While this was a contrived example, it is very common for hyperparameters to greatly affect performance, so always make sure you try tuning your network before declaring failure.\n",
    "\n",
    "#### Callbacks and saving trained networks\n",
    "\n",
    "We are going to retrain the model for a little longer, but first we are going to introduce *callbacks*, which will allow us to periodically save network weights during training so that they can be reloaded at a later time. Callbacks are objects that you can pass to the model at training time which will perform certain tasks at the end of each epoch. In the cell below, we create a [`ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) callback which will save the network parameters to files after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the directory where model weights will be saved. Feel free to change it.\n",
    "CHECKPOINT_DIR = './model-checkpoints/'\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "\n",
    "# The checkpoints will be saved with the corresponding epoch number in their filename\n",
    "ckpt_path = os.path.join(CHECKPOINT_DIR, 'weights.epoch.{epoch:02d}')\n",
    "\n",
    "# Setup checkpoint callback. We only save the weights, not the entire model\n",
    "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(ckpt_path, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model again, this time for 10 epochs and pass our checkpoint callback to the `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_pretrained_vgg_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0175745018cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reset the model again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_pretrained_vgg_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\n\u001b[1;32m      4\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               metrics=['accuracy'])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_pretrained_vgg_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Reset the model again\n",
    "model = build_pretrained_vgg_model()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# The `fit` method returns a dictionary containing metrics from the training.\n",
    "# We will use that information later.\n",
    "history = model.fit(sim_features[:5000],\n",
    "                    sim_targets[:5000],\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[ckpt_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the trained model\n",
    "\n",
    "Before evaluating the model on the real data, we should inspect its *learning curve*. Running the cell below will use the training history to generate a plot of the training loss and validation loss over time. The function call below will generate a learning curve for the model you just trained. Results will vary every time you run the previous cell, but you should see a point at which the training loss passes below the validation loss. This marks the onset of overfitting, which we want to avoid. There are many ways to try to remedy overfitting, such as using more training data, reducing network capacity, or using techniques like [dropout](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer). One technique that should always be used, though, is *early stopping*. Early stopping means that we stop training the model once overfitting begins, i.e. when the training and validation losses diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously we trained too long in this case because the two curves diverge (overfitting started at epoch 3 when I ran this, but will probably be anywhere between 2 and 6), so we will load the network weights that were saved at the point overfitting started before we evaluate the model on the test data. Loading saved weights is easy, and we do so below. Before executing the code, set `EARLY_STOPPING_EPOCH` to the appropriate value based on your learning curve above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EARLY_STOPPING_EPOCH = 0\n",
    "\n",
    "assert EARLY_STOPPING_EPOCH > 0, 'You need to set an early stopping point!'\n",
    "\n",
    "# Path the the checkpoint we want to load\n",
    "es_ckpt_path = os.path.join(CHECKPOINT_DIR, 'weights.epoch.{:02d}'.format(EARLY_STOPPING_EPOCH))\n",
    "\n",
    "# Load the weights from the desired checkpoint into the model\n",
    "model.load_weights(es_ckpt_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally see how our model does on the real AT-TPC data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(real_features, real_targets, verbose=0)\n",
    "\n",
    "print('Test accuracy: {:.04}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number you get could vary, but it shouldn't be anywhere near the validation accuracy. This is not too suprising though. Transfer learning is hard. The network has never seen any of the real data before, and since the simulated data is not entirely accurate, we can't expect the network to know everything about the real data. Supervised learning models are generally very good at interpolation, but not extrapolation.\n",
    "\n",
    "That said, we can dig deeper than just looking at accuracy. In many cases, accuracy might not be what we are most interested in or it can even be misleading. In the case of the Argon 46 experiment, we are most interested in a model that can correctly identify proton events specifically. To get a better idea of how the model performs within each class, we will look at its *confusion matrix*.\n",
    "\n",
    "First we get the model's predictions for all of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model gives us a matrix with each example's softmax probabilities for each class\n",
    "predicted_probabilities = model.predict(real_features)\n",
    "# We reduce that to a single predicted class for each example by taking the\n",
    "# class with the largest probability\n",
    "predictions = np.argmax(predicted_probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of each of the classes\n",
    "class_names = [get_attpc_class(i) for i in range(num_classes)]\n",
    "\n",
    "# This function (defined at the top) will plot the confution matrix.\n",
    "# We give it the predictions and the real labels. The function relies\n",
    "# on scikit-learn's convenient confusion matrix function.\n",
    "plot_confusion_matrix(real_targets, predictions, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your confusion matrix will vary, but it should give you a better idea of how your model performs within each class. From the matrix, we can extract a quantities:\n",
    "\n",
    " * **True Positives (TP):** the correctly predicted positive values, i.e. the examples which have true labels in a given class and were predicted to be in that class.\n",
    " * **True Negatives (TN):** the correctly predicted negative values, i.e. the examples which have true labels not in a given class and were predicted to not be in that class.\n",
    " * **False Positives (FP):** examples which were incorrectly assigned a positive value, i.e. the examples which have true labels not in a given class but were predicted to be in that class.\n",
    " * **False Negatives (FN):** examples which were incorrectly assigned a negative value, i.e. the examples which have true labels in a given class but were predicted to not be in that class.\n",
    " \n",
    "So in the confusion matrix above, the number of true positives for each class are along the diagonal. Using these quantities, we can now define *precision*, *recall*, and *F1-score*.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "    &\\text{precision} = \\dfrac{TP}{TP + FP} \\\\\n",
    "    &\\text{recall} = \\dfrac{TP}{TP + FN} \\\\\n",
    "    &\\text{F1} = \\dfrac{2 \\cdot \\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "These three metrics are much more indicative of a classifier's true performace than accuracy. Intuitively, precision gives us the probability that the model is correct when it classifies something as a certain class. Recall tells us the percentage of examples from a certain class that were correctly classified.\n",
    "\n",
    "We could calculate these by hand from the confusion matrix, but [Scikit-Learn](https://scikit-learn.org/) has the nice [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) function which will do this for us. **In the cell below, replace `None` with a call to `classification_report`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CALL TO CLASSIFICATION_REPORT HERE\n",
    "report = None\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only trained on a few thousand examples with limited hyperparameter tuning, we don't expect great results yet. I challenge you to try to improve these results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn_exercise_solution.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
