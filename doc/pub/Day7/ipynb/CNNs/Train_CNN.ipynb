{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks: Training\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are neural network architectures that contain *filters* that we convolve with inputs to create a new type of layer. \n",
    "\n",
    "<!-- This allows for *feature extraction*, such as the line finding done in our previous examples.-->\n",
    "\n",
    "\n",
    "The convolutional neural network architecture was first described by Kunihiko Fukushima in 1980 (!). With the advent of GPUs in the 2000s, the method became far more popular, with the first implementation on GPUs with backpropogation published in 2011 by Dan C. Cireşan et. al. \n",
    "<!-- in grad school, this was the THING -->\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# activationmn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![simple CNN](CNN_simple.png)\n",
    "\n",
    "We add convolution layers into our NN model to extract features to provide to a fully-connected neural network, therefore, they typically are placed *before* fully connected layers. Our goal is to learn the weights of the fully connected layers **and** the filter values. Therefore, we are not *predefining* the filters, as before, we are *learning* the filters in training.\n",
    "\n",
    "\n",
    "<!-- i hope we saw yesterday that automatic differentiation allows us to define our backprop in a modular way, and swap out portions of the computational graph far more easily than if we were writing out the entire function. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![backpropogation](backprop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backpropogation\n",
    "\n",
    "Our training procedure is exactly the same as previously described for neural networks. However, our mathematics are now different as we backpropogate through the convolutional layers, so let's take a closer look at that for a 1D convolutional filter with a stride of 1.\n",
    "\n",
    "\n",
    "![1D convolution](1d_conv.png)\n",
    "\n",
    "We can write this operation as a computational graph:\n",
    "\n",
    "![computational graph](comp_graph.png)\n",
    "\n",
    "representing:\n",
    "$$\\vec{o}=\\vec{x}\\circledast \\vec{F}$$\n",
    "\n",
    "\n",
    "Where\n",
    "$$o[n]=(x\\circledast F)[n]= 􏰅 \\sum_{i=-\\omega}^{\\omega} x[i+n+\\omega]* F[i+\\omega]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![1D convolution](1d_conv.png)\n",
    "\n",
    "Our goal is to update each $F_i$:\n",
    "$$F_i \\leftarrow F_i - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial F_i}$$\n",
    "\n",
    "Where \n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial F_i} = \\sum_{k=1}^M\\frac{\\partial \\mathcal{L}}{\\partial o_k}\\frac{\\partial o_k}{\\partial F_i}$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's calculate $\\frac{\\partial \\mathcal{L}}{\\partial F_i}$ explicitly for $F_1$ on a smaller architecture:\n",
    "\n",
    "![1D convolution](1d_conv_sm.png)\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial F_i} = \\sum_{k=1}^M\\frac{\\partial \\mathcal{L}}{\\partial o_k}\\frac{\\partial o_k}{\\partial F_i}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial F_1} = \\frac{\\partial \\mathcal{L}}{\\partial o_0}\\frac{\\partial o_0}{\\partial F_1} + \\frac{\\partial \\mathcal{L}}{\\partial o_1}\\frac{\\partial o_1}{\\partial F_1}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![1D convolution](1d_conv_sm.png)\n",
    "\n",
    "We only need to find $\\frac{\\partial o_i}{\\partial F_j}$ terms because, when using automatic differentiation, $\\frac{\\partial \\mathcal{L}}{\\partial o_i}$ is given to us from the right.\n",
    "\n",
    "$$o_0 = F_0x_0 + F_1x_1 $$\n",
    "$$o_1 = F_0x_1 + F_1x_2 $$\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{\\partial o_0}{\\partial F_1} = x_1$$\n",
    "$$\\frac{\\partial o_1}{\\partial F_1} = x_2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "Actually, the backpropogation of a convolutional layer is also a discrete convolution:\n",
    "\n",
    "\n",
    "$$\\vec{x} \\circledast \\frac{\\partial \\mathcal{L}}{\\partial \\vec{o}}$$\n",
    "\n",
    "If we have layers *before* this one, we will also need to compute $\\frac{\\partial \\mathcal{L}}{\\partial x_i}$ to continue the back propogation.\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial x_i} = \\sum_{k=1}^M\\frac{\\partial \\mathcal{L}}{\\partial o_k}\\frac{\\partial o_k}{\\partial x_i}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Activation function\n",
    "\n",
    "Typically, an activation function is then applied to *each element* the feature map. ReLU is the most common for CNNs, as they are typically deep structures.\n",
    "\n",
    "![ReLU](relu.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![simple CNN](CNN_simple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pooling\n",
    "\n",
    "![pooling](pooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GoogLeNet\n",
    "\n",
    "![GoogLeNet](googlenet.png)\n",
    "\n",
    "\n",
    "<!-- depthconcat -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Umm, how do I pick an architecture?\n",
    "\n",
    "How many layers? \n",
    "\n",
    "How many nodes per layer?\n",
    "\n",
    "How many convolution layers?\n",
    "\n",
    "What activation function(s)?\n",
    "\n",
    "Dropout...Y/N?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Pre-trained architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "| CNN Architectures  |   |\n",
    "|---|---|\n",
    "|  AlexNet |   |\n",
    "| VGG  | \n",
    "|  ResNets |   |\n",
    "|  Inception |   |   \n",
    "|  Xception |   |   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "img_path = 'landis.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "plt.imshow(x)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
