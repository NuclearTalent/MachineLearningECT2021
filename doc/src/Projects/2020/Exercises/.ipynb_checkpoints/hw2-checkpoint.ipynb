{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Day 3: Homework 2 -->\n",
    "# Day 3: Homework 2\n",
    "<!-- dom:AUTHOR: Data Analysis and Machine Learning -->\n",
    "<!-- Author: -->  \n",
    "**Data Analysis and Machine Learning**\n",
    "\n",
    "Date: **May 22, 2020**\n",
    "\n",
    "## Day three exercises\n",
    "\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "This exercise is a continuation of exercise 3 from homework 1. We will\n",
    "use the same function to generate our data set, still staying with a\n",
    "simple function $y(x)$ which we want to fit using linear regression,\n",
    "but now extending the analysis to include the Ridge and the Lasso\n",
    "regression methods. You can use the code under the Regression as an example on how to use the Ridge and the Lasso methods, see the [regression slides](https://compphysics.github.io/MachineLearning/doc/pub/Regression/html/Regression-bs.html)). \n",
    "\n",
    "We will thus again generate our own dataset for a function $y(x)$ where \n",
    "$x \\in [0,1]$ and defined by random numbers computed with the uniform\n",
    "distribution. The function $y$ is a quadratic polynomial in $x$ with\n",
    "added stochastic noise according to the normal distribution $\\cal{N}(0,1)$.\n",
    "\n",
    "The following simple Python instructions define our $x$ and $y$ values (with 100 data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100,1)\n",
    "y = 5*x*x+0.1*np.random.randn(100,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (1a) Write your own code for the Ridge method (see chapter 3.4 of Hastie *et al.*, equations (3.43) and (3.44)) and compute the parametrization for different values of $\\lambda$. Compare and analyze your results with those from exercise 2. Study the dependence on $\\lambda$ while also varying the strength of the noise in your expression for $y(x)$. \n",
    "\n",
    "2. (1b) Repeat the above but using the functionality of **Scikit-Learn**. Compare your code with the results from **Scikit-Learn**. Remember to run with the same random numbers for generating $x$ and $y$. \n",
    "\n",
    "3. (1c) Our next step is to study the variance of the parameters $\\beta_1$ and $\\beta_2$ (assuming that we are parameterizing our function with a second-order polynomial). We will use standard linear regression and the Ridge regression.  You can now opt for either writing your own function or using **Scikit-Learn** to find the parameters $\\beta$. From your results calculate the variance of these paramaters (recall that this is equal to the diagonal elements of the matrix $(\\hat{X}^T\\hat{X})+\\lambda\\hat{I})^{-1}$). Discuss the results of these variances as functions of $\\lambda$. In particular, try to link your discussion with the discussion in Hastie *et al.* and their figure 3.11.\n",
    "\n",
    "4. (1d) Repeat the previous step but add now the Lasso method, see equation (3.53) of Hastie *et al.*. Discuss your results and compare with standard regression and the Ridge regression results. You can write your own code or use the functionality of **scikit-learn**.  We recommend the latter since we have not yet discussed how to solve the Lasso equations numerically.\n",
    "\n",
    "5. (1e) Finally, using **Scikit-Learn** or your own code, compute also the mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE(\\hat{y},\\hat{\\tilde{y}}) = \\frac{1}{n}\n",
    "\\sum_{i=0}^{n-1}(y_i-\\tilde{y}_i)^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the $R^2$ score function.\n",
    "If $\\tilde{\\hat{y}}_i$ is the predicted value of the $i-th$ sample and $y_i$ is the corresponding true value, then the score $R^2$ is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "R^2(\\hat{y}, \\tilde{\\hat{y}}) = 1 - \\frac{\\sum_{i=0}^{n - 1} (y_i - \\tilde{y}_i)^2}{\\sum_{i=0}^{n - 1} (y_i - \\bar{y})^2},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we have defined the mean value  of $\\hat{y}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bar{y} =  \\frac{1}{n} \\sum_{i=0}^{n - 1} y_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss these quantities as functions of the variable $\\lambda$ in the Ridge and Lasso regression methods. \n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "\n",
    "A much used approach before starting to train the data is  to preprocess our\n",
    "data. Normally the data may need a rescaling and/or may be sensitive\n",
    "to extreme values. Scaling the data renders our inputs much more\n",
    "suitable for the algorithms we want to employ.\n",
    "\n",
    "**Scikit-Learn** has several functions which allow us to rescale the\n",
    "data, normally resulting in much better results in terms of various\n",
    "accuracy scores.  The **StandardScaler** function in **Scikit-Learn**\n",
    "ensures that for each feature/predictor we study the mean value is\n",
    "zero and the variance is one (every column in the design/feature\n",
    "matrix).  This scaling has the drawback that it does not ensure that\n",
    "we have a particular maximum or minimum in our data set. Another\n",
    "function included in **Scikit-Learn** is the **MinMaxScaler** which\n",
    "ensures that all features are exactly between $0$ and $1$. The\n",
    "\n",
    "\n",
    "The **Normalizer** scales each data\n",
    "point such that the feature vector has a euclidean length of one. In other words, it\n",
    "projects a data point on the circle (or sphere in the case of higher dimensions) with a\n",
    "radius of 1. This means every data point is scaled by a different number (by the\n",
    "inverse of itâ€™s length).\n",
    "This normalization is often used when only the direction (or angle) of the data matters,\n",
    "not the length of the feature vector.\n",
    "\n",
    "The **RobustScaler** works similarly to the StandardScaler in that it\n",
    "ensures statistical properties for each feature that guarantee that\n",
    "they are on the same scale. However, the RobustScaler uses the median\n",
    "and quartiles, instead of mean and variance. This makes the\n",
    "RobustScaler ignore data points that are very different from the rest\n",
    "(like measurement errors). These odd data points are also called\n",
    "outliers, and might often lead to trouble for other scaling\n",
    "techniques.\n",
    "\n",
    "\n",
    "It also common to split the data in a **training** set and a **testing** set. A typical split is to use $80\\%$ of the data for training and the rest\n",
    "for testing. This can be done as follows with our design matrix $\\boldsymbol{X}$ and data $\\boldsymbol{y}$ (remember to import **scikit-learn**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the stadndard scale to scale our data as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we want you to to compute the MSE for the training\n",
    "data and the test data as function of the complexity of a polynomial,\n",
    "that is the degree of a given polynomial. We want you also to compute the $R2$ score as function of the complexity of the model for both training data and test data.  You should also run the calculation with and without scaling. \n",
    "\n",
    "One of \n",
    "the aims is to reproduce Figure 2.11 of [Hastie et al](https://github.com/CompPhysics/MLErasmus/blob/master/doc/Textbooks/elementsstat.pdf).\n",
    "We will also use Ridge and Lasso regression. \n",
    "\n",
    "\n",
    "Our data is defined by $x\\in [-3,3]$ with a total of for example $100$ data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed()\n",
    "n = 100\n",
    "maxdegree = 14\n",
    "# Make data set.\n",
    "x = np.linspace(-3, 3, n).reshape(-1, 1)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2)+ np.random.normal(0, 0.1, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $y$ is the function we want to fit with a given polynomial.\n",
    "\n",
    "1. (2a) Write a first code which sets up a design matrix $X$ defined by a fifth-order polynomial.  Scale your data and split it in training and test data. \n",
    "\n",
    "2. (2b) Perform an ordinary least squares and compute the means squared error and the $R2$ factor for the training data and the test data, with and without scaling.\n",
    "\n",
    "3. (2c) Add now a model which allows you to make polynomials up to degree $15$.  Perform a standard OLS fitting of the training data and compute the MSE and $R2$ for the training and test data and plot both test and training data MSE and $R2$ as functions of the polynomial degree. Compare what you see with Figure 2.11 of Hastie et al. Comment your results. For which polynomial degree do you find an optimal MSE (smallest value)?\n",
    "\n",
    "4. (2d) Repeat part (2c) but now using Ridge regressions with various hyperparameters $\\lambda$. Make the same plots for the optimal $\\lambda$ value for each polynomial degree. Compare these results with those from the standard OLS approach.\n",
    "\n",
    "## Example of how to solve the previous exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "n = 100\n",
    "# Make data set.\n",
    "x = np.linspace(-3, 3, n).reshape(-1, 1)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2)+ np.random.normal(0, 0.1, x.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Decide which values of lambda to use\n",
    "nlambdas = 500\n",
    "lambdas = np.logspace(-3, 5, nlambdas)\n",
    "\n",
    "\n",
    "estimated_mse_sklearn = np.zeros(nlambdas)\n",
    "i = 0\n",
    "for lmb in lambdas:\n",
    "    clf_ridge = Ridge(alpha=lmb).fit(X_train_scaled, y_train)\n",
    "    yridge = clf_ridge.predict(X_test_scaled)\n",
    "    estimated_mse_sklearn[i] = mean_squared_error(y_test, yridge)\n",
    "    i += 1\n",
    "plt.figure()\n",
    "plt.plot(np.log10(lambdas), estimated_mse_sklearn, label = 'Ridge MSE')\n",
    "plt.xlabel('log10(lambda)')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now with OLS only and Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e6484d6b1f14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdegree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPolynomialFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_boostraps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_boostraps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "n = 100\n",
    "n_boostraps = 100\n",
    "maxdegree = 14\n",
    "\n",
    "# Make data set.\n",
    "x = np.linspace(-3, 3, n).reshape(-1, 1)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2)+ np.random.normal(0, 0.1, x.shape)\n",
    "error = np.zeros(maxdegree)\n",
    "bias = np.zeros(maxdegree)\n",
    "variance = np.zeros(maxdegree)\n",
    "polydegree = np.zeros(maxdegree)\n",
    "\n",
    "\n",
    "# Make data set.\n",
    "x = np.linspace(-3, 3, n).reshape(-1, 1)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2)+ np.random.normal(0, 0.1, x.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "for degree in range(maxdegree):\n",
    "    model = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression(fit_intercept=False))\n",
    "    y_pred = np.empty((y_test.shape[0], n_boostraps))\n",
    "    for i in range(n_boostraps):\n",
    "        x_, y_ = resample(x_train_scaled, y_train)\n",
    "        y_pred[:, i] = model.fit(x_, y_).predict(x_test_scaled).ravel()\n",
    "\n",
    "    polydegree[degree] = degree\n",
    "    error[degree] = np.mean( np.mean((y_test - y_pred)**2, axis=1, keepdims=True) )\n",
    "    bias[degree] = np.mean( (y_test - np.mean(y_pred, axis=1, keepdims=True))**2 )\n",
    "    variance[degree] = np.mean( np.var(y_pred, axis=1, keepdims=True) )\n",
    "    print('Polynomial degree:', degree)\n",
    "    print('Error:', error[degree])\n",
    "    print('Bias^2:', bias[degree])\n",
    "    print('Var:', variance[degree])\n",
    "    print('{} >= {} + {} = {}'.format(error[degree], bias[degree], variance[degree], bias[degree]+variance[degree]))\n",
    "\n",
    "plt.plot(polydegree, error, label='Error')\n",
    "plt.plot(polydegree, bias, label='bias')\n",
    "plt.plot(polydegree, variance, label='Variance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
